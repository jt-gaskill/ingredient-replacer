{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d45d758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/groups/accompaniments\n",
      "/groups/baked-goods\n",
      "/groups/baking-supplies\n",
      "/groups/dairy\n",
      "/groups/equipment\n",
      "/groups/fats-oils\n",
      "/groups/fish\n",
      "/groups/flavorings\n",
      "/groups/fruit\n",
      "/groups/grain-products\n",
      "/groups/grains\n",
      "/groups/legumes-nuts\n",
      "/groups/liquids\n",
      "/groups/meats\n",
      "/groups/miscellaneous\n",
      "/groups/vegetables\n",
      "/groups/vegetarian\n",
      "Found 17 category pages.\n",
      "https://foodsubs.com/groups/baked-goods\n",
      "https://foodsubs.com/groups/fats-oils\n",
      "https://foodsubs.com/groups/grains\n",
      "https://foodsubs.com/groups/vegetarian\n",
      "https://foodsubs.com/groups/grain-products\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "BASE_URL = \"https://foodsubs.com/groups\"\n",
    "\n",
    "def get_category_links():\n",
    "    res = requests.get(BASE_URL)\n",
    "    if res.status_code != 200:\n",
    "        print(f\"Failed to retrieve {BASE_URL}. Status code: {res.status_code}\")\n",
    "        return []\n",
    "    \n",
    "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "    \n",
    "    # Find the div with class 'hero-groups' which contains ingredient groups\n",
    "    hero_groups = soup.find(\"div\", class_=\"hero-groups\")\n",
    "    \n",
    "    if not hero_groups:\n",
    "        print(\"Could not find 'hero-groups' div. Structure may have changed.\")\n",
    "        return []\n",
    "    \n",
    "    # Find all the <a> tags with the ingredient links inside this div\n",
    "    links = hero_groups.find_all(\"a\", href=True)\n",
    "    \n",
    "    category_links = []\n",
    "    for link in links:\n",
    "        href = link['href']\n",
    "        print(href)  # Print all hrefs to inspect the structure\n",
    "        \n",
    "        # Check if the href represents a valid category page (adjust logic if necessary)\n",
    "        # For example, you could look for specific patterns like '/groups/', '/categories/', etc.\n",
    "        if \"/groups/\" in href:\n",
    "            full_url = href if href.startswith(\"http\") else f\"{BASE_URL}{href}\"\n",
    "            category_links.append(full_url)\n",
    "\n",
    "    return list(set(category_links))  # Deduplicate to avoid any duplicate links\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    category_links = get_category_links()\n",
    "    print(f\"Found {len(category_links)} category pages.\")\n",
    "    for link in category_links[:5]:  # Print first 5 links\n",
    "        print(link)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409f1c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/groups/accompaniments\n",
      "/groups/baked-goods\n",
      "/groups/baking-supplies\n",
      "/groups/dairy\n",
      "/groups/equipment\n",
      "/groups/fats-oils\n",
      "/groups/fish\n",
      "/groups/flavorings\n",
      "/groups/fruit\n",
      "/groups/grain-products\n",
      "/groups/grains\n",
      "/groups/legumes-nuts\n",
      "/groups/liquids\n",
      "/groups/meats\n",
      "/groups/miscellaneous\n",
      "/groups/vegetables\n",
      "/groups/vegetarian\n",
      "Scraping https://foodsubs.com/groups/baked-goods\n",
      "Scraping https://foodsubs.com/groups/fats-oils\n",
      "Scraping https://foodsubs.com/groups/grains\n",
      "Scraping https://foodsubs.com/groups/vegetarian\n",
      "Scraping https://foodsubs.com/groups/grain-products\n",
      "Scraping https://foodsubs.com/groups/fruit\n",
      "Scraping https://foodsubs.com/groups/vegetables\n",
      "Scraping https://foodsubs.com/groups/dairy\n",
      "Scraping https://foodsubs.com/groups/legumes-nuts\n",
      "Scraping https://foodsubs.com/groups/flavorings\n",
      "Scraping https://foodsubs.com/groups/liquids\n",
      "Scraping https://foodsubs.com/groups/baking-supplies\n",
      "Scraping https://foodsubs.com/groups/meats\n",
      "Scraping https://foodsubs.com/groups/miscellaneous\n",
      "Scraping https://foodsubs.com/groups/fish\n",
      "Scraping https://foodsubs.com/groups/accompaniments\n",
      "Scraping https://foodsubs.com/groups/equipment\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "def extract_ingredients_from_page(url):\n",
    "    res = requests.get(url)\n",
    "    if res.status_code != 200:\n",
    "        print(f\"Failed to retrieve {url}. Status code: {res.status_code}\")\n",
    "        return []\n",
    "    \n",
    "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "\n",
    "    data = []\n",
    "\n",
    "    paragraphs = soup.find_all(['p', 'ul'])\n",
    "    for p in paragraphs:\n",
    "        text = p.get_text().strip()\n",
    "        if not text:\n",
    "            continue\n",
    "\n",
    "        # Try to identify substitutions\n",
    "        match = re.search(r\"(?P<name>^[^:.]+?)\\s*(?:[:\\-â€“â€”])?\\s*(?:Substitute[s]?:?\\s*(?P<subs>.*?))(?:\\.\\s|$)\", text, re.IGNORECASE)\n",
    "        if match:\n",
    "            ingredient = match.group(\"name\").strip()\n",
    "            subs = match.group(\"subs\").strip()\n",
    "            data.append({\n",
    "                \"ingredient\": ingredient,\n",
    "                \"substitutes\": subs,\n",
    "                \"may_include\": re.findall(r\"\\(.*?may include.*?\\)\", text, re.IGNORECASE)\n",
    "            })\n",
    "\n",
    "    return data\n",
    "\n",
    "def scrape_foodsubs():\n",
    "    category_links = get_category_links()\n",
    "    all_ingredients = []\n",
    "\n",
    "    for link in category_links:\n",
    "        print(f\"Scraping {link}\")\n",
    "        try:\n",
    "            ingredients = extract_ingredients_from_page(link)\n",
    "            all_ingredients.extend(ingredients)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to scrape {link}: {e}\")\n",
    "        time.sleep(1)  # Be respectful\n",
    "    \n",
    "    return all_ingredients\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d5e60579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/groups/accompaniments\n",
      "/groups/baked-goods\n",
      "/groups/baking-supplies\n",
      "/groups/dairy\n",
      "/groups/equipment\n",
      "/groups/fats-oils\n",
      "/groups/fish\n",
      "/groups/flavorings\n",
      "/groups/fruit\n",
      "/groups/grain-products\n",
      "/groups/grains\n",
      "/groups/legumes-nuts\n",
      "/groups/liquids\n",
      "/groups/meats\n",
      "/groups/miscellaneous\n",
      "/groups/vegetables\n",
      "/groups/vegetarian\n",
      "Scraping https://foodsubs.com/groups/baked-goods\n",
      "Scraping https://foodsubs.com/groups/fats-oils\n",
      "Scraping https://foodsubs.com/groups/grains\n",
      "Scraping https://foodsubs.com/groups/vegetarian\n",
      "Scraping https://foodsubs.com/groups/grain-products\n",
      "Scraping https://foodsubs.com/groups/fruit\n",
      "Scraping https://foodsubs.com/groups/vegetables\n",
      "Scraping https://foodsubs.com/groups/dairy\n",
      "Scraping https://foodsubs.com/groups/legumes-nuts\n",
      "Scraping https://foodsubs.com/groups/flavorings\n",
      "Scraping https://foodsubs.com/groups/liquids\n",
      "Scraping https://foodsubs.com/groups/baking-supplies\n",
      "Scraping https://foodsubs.com/groups/meats\n",
      "Scraping https://foodsubs.com/groups/miscellaneous\n",
      "Scraping https://foodsubs.com/groups/fish\n",
      "Scraping https://foodsubs.com/groups/accompaniments\n",
      "Scraping https://foodsubs.com/groups/equipment\n",
      "Results saved to ingredient_substitutions.json\n"
     ]
    }
   ],
   "source": [
    "# Execute the scraping\n",
    "results = scrape_foodsubs()\n",
    "\n",
    "# Saving the results to a file (e.g., JSON)\n",
    "with open('ingredient_substitutions.json', 'w') as f:\n",
    "    json.dump(results, f, indent=4)\n",
    "\n",
    "print(\"Results saved to ingredient_substitutions.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d8bca2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting BeautifulSoup4\n",
      "  Downloading beautifulsoup4-4.13.3-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting soupsieve>1.2 (from BeautifulSoup4)\n",
      "  Using cached soupsieve-2.6-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in ./.conda/lib/python3.12/site-packages (from BeautifulSoup4) (4.13.2)\n",
      "Downloading beautifulsoup4-4.13.3-py3-none-any.whl (186 kB)\n",
      "Using cached soupsieve-2.6-py3-none-any.whl (36 kB)\n",
      "Installing collected packages: soupsieve, BeautifulSoup4\n",
      "Successfully installed BeautifulSoup4-4.13.3 soupsieve-2.6\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install BeautifulSoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "320273ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 61 ingredient list pages.\n",
      "Scraping https://foodsubs.com/groups/vegetables/tubers-corms?ingredients\n",
      "Scraping https://foodsubs.com/groups/meats/veal?ingredients\n",
      "Scraping https://foodsubs.com/groups/grains?ingredients\n",
      "Scraping https://foodsubs.com/groups/fruit?ingredients\n",
      "Scraping https://foodsubs.com/groups/vegetables/fruit-vegetables/dried-chili-peppers?ingredients\n",
      "Scraping https://foodsubs.com/groups/flavorings/wine?ingredients\n",
      "Scraping https://foodsubs.com/groups/meats/beef?ingredients\n",
      "Scraping https://foodsubs.com/groups/meats/variety-meats?ingredients\n",
      "Scraping https://foodsubs.com/groups/meats?ingredients\n",
      "Scraping https://foodsubs.com/groups/fish/shellfish?ingredients\n",
      "Scraping https://foodsubs.com/groups/baking-supplies/leavens?ingredients\n",
      "Scraping https://foodsubs.com/groups/fish/shellfish/crabs-and-shrimp?ingredients\n",
      "Scraping https://foodsubs.com/groups/accompaniments/fruit-preserves?ingredients\n",
      "Scraping https://foodsubs.com/groups/liquids/alcohol/wine?ingredients\n",
      "Scraping https://foodsubs.com/groups/vegetarian?ingredients\n",
      "Scraping https://foodsubs.com/groups/flavorings/spices?ingredients\n",
      "Scraping https://foodsubs.com/groups/meats/pork?ingredients\n",
      "Scraping https://foodsubs.com/groups/accompaniments/condiments?ingredients\n",
      "Scraping https://foodsubs.com/groups/flavorings?ingredients\n",
      "Scraping https://foodsubs.com/groups/fruit/fruit-vegetables/cucumbers?ingredients\n",
      "Scraping https://foodsubs.com/groups/flavorings/liquid-sweeteners?ingredients\n",
      "Scraping https://foodsubs.com/groups/fruit/exotic-tropical-fruit?ingredients\n",
      "Scraping https://foodsubs.com/groups/fruit/fruit-vegetables?ingredients\n",
      "Scraping https://foodsubs.com/groups/grain-products/thickeners?ingredients\n",
      "Scraping https://foodsubs.com/groups/fruit/dried-fruit?ingredients\n",
      "Scraping https://foodsubs.com/groups/fish/lean-flaky-textured-fish?ingredients\n",
      "Scraping https://foodsubs.com/groups/liquids/alcohol/liquors?ingredients\n",
      "Scraping https://foodsubs.com/groups/vegetables/fruit-vegetables?ingredients\n",
      "Scraping https://foodsubs.com/groups/liquids?ingredients\n",
      "Scraping https://foodsubs.com/groups/flavorings/condiments?ingredients\n",
      "Scraping https://foodsubs.com/groups/equipment?ingredients\n",
      "Scraping https://foodsubs.com/groups/liquids/alcohol/liqueurs?ingredients\n",
      "Scraping https://foodsubs.com/groups/vegetables/onions?ingredients\n",
      "Scraping https://foodsubs.com/groups/fruit/fruit-preserves?ingredients\n",
      "Scraping https://foodsubs.com/groups/grain-products/pasta?ingredients\n",
      "Scraping https://foodsubs.com/groups/flavorings/herbs?ingredients\n",
      "Scraping https://foodsubs.com/groups/vegetables/fruit-vegetables/fresh-chili-peppers?ingredients\n",
      "Scraping https://foodsubs.com/groups/fruit/pome-fruit?ingredients\n",
      "Scraping https://foodsubs.com/groups/fish/fatty-firm-textured-fish?ingredients\n",
      "Scraping https://foodsubs.com/groups/miscellaneous/thickeners?ingredients\n",
      "Scraping https://foodsubs.com/groups/grain-products?ingredients\n",
      "Scraping https://foodsubs.com/groups/liquids/alcohol?ingredients\n",
      "Scraping https://foodsubs.com/groups/vegetables?ingredients\n",
      "Scraping https://foodsubs.com/groups/legumes-nuts?ingredients\n",
      "Scraping https://foodsubs.com/groups/grain-products/asian-noodles?ingredients\n",
      "Scraping https://foodsubs.com/groups/accompaniments?ingredients\n",
      "Scraping https://foodsubs.com/groups/flavorings/herb-spice-mixes?ingredients\n",
      "Scraping https://foodsubs.com/groups/miscellaneous?ingredients\n",
      "Scraping https://foodsubs.com/groups/vegetables/fruit-vegetables/cucumbers?ingredients\n",
      "Scraping https://foodsubs.com/groups/vegetarian/plant-based-meat?ingredients\n",
      "Scraping https://foodsubs.com/groups/flavorings/fruit-preserves?ingredients\n",
      "Scraping https://foodsubs.com/groups/flavorings/spices/global-spices?ingredients\n",
      "Scraping https://foodsubs.com/groups/baked-goods?ingredients\n",
      "Scraping https://foodsubs.com/groups/meats/lamb?ingredients\n",
      "Scraping https://foodsubs.com/groups/dairy/cheese?ingredients\n",
      "Scraping https://foodsubs.com/groups/dairy?ingredients\n",
      "Scraping https://foodsubs.com/groups/meats/cured-meats?ingredients\n",
      "Scraping https://foodsubs.com/groups/fish?ingredients\n",
      "Scraping https://foodsubs.com/groups/fish/fatty-flaky-textured-fish?ingredients\n",
      "Scraping https://foodsubs.com/groups/fats-oils?ingredients\n",
      "Scraping https://foodsubs.com/groups/baking-supplies?ingredients\n",
      "\n",
      "ğŸ” Found 1435 ingredient detail pages.\n",
      "\n",
      "âœ… Scraped 0 ingredients. Results saved to ingredient_substitutions.json\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import re\n",
    "import json\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "BASE_URL = \"https://foodsubs.com/groups\"\n",
    "\n",
    "# Step 1: Get links to the full ingredient lists in each group\n",
    "def get_ingredient_links():\n",
    "    res = requests.get(BASE_URL)\n",
    "    if res.status_code != 200:\n",
    "        print(f\"Failed to retrieve {BASE_URL}. Status code: {res.status_code}\")\n",
    "        return []\n",
    "\n",
    "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "    groups_menu = soup.find(\"div\", class_=\"groups-menu\")\n",
    "\n",
    "    if not groups_menu:\n",
    "        print(\"Could not find 'groups-menu' div.\")\n",
    "        return []\n",
    "\n",
    "    ingredient_links = []\n",
    "    anchors = groups_menu.find_all(\"a\", class_=\"list-group-item-see-all\")\n",
    "    for a in anchors:\n",
    "        href = a.get(\"href\")\n",
    "        if href:\n",
    "            full_url = href if href.startswith(\"http\") else f\"https://foodsubs.com{href}\"\n",
    "            ingredient_links.append(full_url)\n",
    "\n",
    "    print(f\"Found {len(ingredient_links)} ingredient list pages.\")\n",
    "    return list(set(ingredient_links))  # Deduplicate\n",
    "\n",
    "# Step 2: Visit each ingredient list page and extract substitution info\n",
    "def extract_ingredients_from_page(url):\n",
    "    res = requests.get(url)\n",
    "    if res.status_code != 200:\n",
    "        print(f\"Failed to retrieve {url}. Status code: {res.status_code}\")\n",
    "        return [], []\n",
    "\n",
    "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "    data = []\n",
    "    ingredient_urls = []\n",
    "\n",
    "    # Pull substitution info from paragraphs and lists\n",
    "    paragraphs = soup.find_all(['p', 'ul'])\n",
    "    for p in paragraphs:\n",
    "        text = p.get_text().strip()\n",
    "        if not text:\n",
    "            continue\n",
    "\n",
    "        match = re.search(r\"(?P<name>^[^:.]+?)\\s*(?:[:\\-â€“â€”])?\\s*(?:Substitute[s]?:?\\s*(?P<subs>.*?))(?:\\.\\s|$)\", text, re.IGNORECASE)\n",
    "        if match:\n",
    "            ingredient = match.group(\"name\").strip()\n",
    "            subs = match.group(\"subs\").strip()\n",
    "            data.append({\n",
    "                \"ingredient\": ingredient,\n",
    "                \"substitutes\": subs,\n",
    "                \"source_url\": url\n",
    "            })\n",
    "\n",
    "    # âœ… Grab \"Learn more\" ingredient detail links\n",
    "    anchors = soup.select(\"a.card-learn-more[href*='/ingredients/']\")\n",
    "    for a in anchors:\n",
    "        href = a.get(\"href\")\n",
    "        if href:\n",
    "            full_url = href if href.startswith(\"http\") else f\"https://foodsubs.com{href}\"\n",
    "            ingredient_urls.append(full_url)\n",
    "\n",
    "    return data, list(set(ingredient_urls))  # Return both substitution data and ingredient links\n",
    "\n",
    "\n",
    "# Step 3: Scrape all pages\n",
    "def scrape_foodsubs():\n",
    "    ingredient_pages = get_ingredient_links()\n",
    "    all_ingredients = []\n",
    "    all_detail_links = []\n",
    "\n",
    "    for link in ingredient_pages:\n",
    "        print(f\"Scraping {link}\")\n",
    "        try:\n",
    "            ingredients, detail_links = extract_ingredients_from_page(link)\n",
    "            all_ingredients.extend(ingredients)\n",
    "            all_detail_links.extend(detail_links)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to scrape {link}: {e}\")\n",
    "        time.sleep(1)  # Be respectful to the server\n",
    "\n",
    "    all_detail_links = list(set(all_detail_links))  # Deduplicate if you plan to use them\n",
    "    print(f\"\\nğŸ” Found {len(all_detail_links)} ingredient detail pages.\")\n",
    "    return all_ingredients\n",
    "\n",
    "\n",
    "# Step 4: Save results to JSON\n",
    "if __name__ == \"__main__\":\n",
    "    results = scrape_foodsubs()\n",
    "\n",
    "    with open('ingredient_substitutions.json', 'w') as f:\n",
    "        json.dump(results, f, indent=4)\n",
    "\n",
    "    print(f\"\\nâœ… Scraped {len(results)} ingredients. Results saved to ingredient_substitutions.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b3b740c",
   "metadata": {},
   "outputs": [
    {
     "ename": "Error",
     "evalue": "BrowserType.launch: Executable doesn't exist at /Users/chengling/Library/Caches/ms-playwright/chromium_headless_shell-1161/chrome-mac/headless_shell\nâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\nâ•‘ Looks like Playwright was just installed or updated.       â•‘\nâ•‘ Please run the following command to download new browsers: â•‘\nâ•‘                                                            â•‘\nâ•‘     playwright install                                     â•‘\nâ•‘                                                            â•‘\nâ•‘ <3 Playwright Team                                         â•‘\nâ•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mError\u001b[39m                                     Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     23\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m browser.close()\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# Run the function within the existing event loop\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m scrape_foodsubs()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 10\u001b[39m, in \u001b[36mscrape_foodsubs\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mscrape_foodsubs\u001b[39m():\n\u001b[32m      9\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m async_playwright() \u001b[38;5;28;01mas\u001b[39;00m p:\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m         browser = \u001b[38;5;28;01mawait\u001b[39;00m p.chromium.launch(headless=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     11\u001b[39m         page = \u001b[38;5;28;01mawait\u001b[39;00m browser.new_page()\n\u001b[32m     12\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m page.goto(\u001b[33m\"\u001b[39m\u001b[33mhttps://foodsubs.com/groups\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/ingredient-replacer/.conda/lib/python3.12/site-packages/playwright/async_api/_generated.py:14450\u001b[39m, in \u001b[36mBrowserType.launch\u001b[39m\u001b[34m(self, executable_path, channel, args, ignore_default_args, handle_sigint, handle_sigterm, handle_sighup, timeout, env, headless, devtools, proxy, downloads_path, slow_mo, traces_dir, chromium_sandbox, firefox_user_prefs)\u001b[39m\n\u001b[32m  14332\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlaunch\u001b[39m(\n\u001b[32m  14333\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m  14334\u001b[39m     *,\n\u001b[32m   (...)\u001b[39m\u001b[32m  14355\u001b[39m     ] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m  14356\u001b[39m ) -> \u001b[33m\"\u001b[39m\u001b[33mBrowser\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m  14357\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"BrowserType.launch\u001b[39;00m\n\u001b[32m  14358\u001b[39m \n\u001b[32m  14359\u001b[39m \u001b[33;03m    Returns the browser instance.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m  14446\u001b[39m \u001b[33;03m    Browser\u001b[39;00m\n\u001b[32m  14447\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m  14449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m mapping.from_impl(\n\u001b[32m> \u001b[39m\u001b[32m14450\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._impl_obj.launch(\n\u001b[32m  14451\u001b[39m             executablePath=executable_path,\n\u001b[32m  14452\u001b[39m             channel=channel,\n\u001b[32m  14453\u001b[39m             args=mapping.to_impl(args),\n\u001b[32m  14454\u001b[39m             ignoreDefaultArgs=mapping.to_impl(ignore_default_args),\n\u001b[32m  14455\u001b[39m             handleSIGINT=handle_sigint,\n\u001b[32m  14456\u001b[39m             handleSIGTERM=handle_sigterm,\n\u001b[32m  14457\u001b[39m             handleSIGHUP=handle_sighup,\n\u001b[32m  14458\u001b[39m             timeout=timeout,\n\u001b[32m  14459\u001b[39m             env=mapping.to_impl(env),\n\u001b[32m  14460\u001b[39m             headless=headless,\n\u001b[32m  14461\u001b[39m             devtools=devtools,\n\u001b[32m  14462\u001b[39m             proxy=proxy,\n\u001b[32m  14463\u001b[39m             downloadsPath=downloads_path,\n\u001b[32m  14464\u001b[39m             slowMo=slow_mo,\n\u001b[32m  14465\u001b[39m             tracesDir=traces_dir,\n\u001b[32m  14466\u001b[39m             chromiumSandbox=chromium_sandbox,\n\u001b[32m  14467\u001b[39m             firefoxUserPrefs=mapping.to_impl(firefox_user_prefs),\n\u001b[32m  14468\u001b[39m         )\n\u001b[32m  14469\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/ingredient-replacer/.conda/lib/python3.12/site-packages/playwright/_impl/_browser_type.py:96\u001b[39m, in \u001b[36mBrowserType.launch\u001b[39m\u001b[34m(self, executablePath, channel, args, ignoreDefaultArgs, handleSIGINT, handleSIGTERM, handleSIGHUP, timeout, env, headless, devtools, proxy, downloadsPath, slowMo, tracesDir, chromiumSandbox, firefoxUserPrefs)\u001b[39m\n\u001b[32m     93\u001b[39m params = locals_to_params(\u001b[38;5;28mlocals\u001b[39m())\n\u001b[32m     94\u001b[39m normalize_launch_params(params)\n\u001b[32m     95\u001b[39m browser = cast(\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m     Browser, from_channel(\u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._channel.send(\u001b[33m\"\u001b[39m\u001b[33mlaunch\u001b[39m\u001b[33m\"\u001b[39m, params))\n\u001b[32m     97\u001b[39m )\n\u001b[32m     98\u001b[39m \u001b[38;5;28mself\u001b[39m._did_launch_browser(browser)\n\u001b[32m     99\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m browser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/ingredient-replacer/.conda/lib/python3.12/site-packages/playwright/_impl/_connection.py:61\u001b[39m, in \u001b[36mChannel.send\u001b[39m\u001b[34m(self, method, params)\u001b[39m\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msend\u001b[39m(\u001b[38;5;28mself\u001b[39m, method: \u001b[38;5;28mstr\u001b[39m, params: Dict = \u001b[38;5;28;01mNone\u001b[39;00m) -> Any:\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._connection.wrap_api_call(\n\u001b[32m     62\u001b[39m         \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28mself\u001b[39m._inner_send(method, params, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[32m     63\u001b[39m         \u001b[38;5;28mself\u001b[39m._is_internal_type,\n\u001b[32m     64\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/ingredient-replacer/.conda/lib/python3.12/site-packages/playwright/_impl/_connection.py:528\u001b[39m, in \u001b[36mConnection.wrap_api_call\u001b[39m\u001b[34m(self, cb, is_internal)\u001b[39m\n\u001b[32m    526\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m cb()\n\u001b[32m    527\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[32m--> \u001b[39m\u001b[32m528\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m rewrite_error(error, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparsed_st[\u001b[33m'\u001b[39m\u001b[33mapiName\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    529\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    530\u001b[39m     \u001b[38;5;28mself\u001b[39m._api_zone.set(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[31mError\u001b[39m: BrowserType.launch: Executable doesn't exist at /Users/chengling/Library/Caches/ms-playwright/chromium_headless_shell-1161/chrome-mac/headless_shell\nâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\nâ•‘ Looks like Playwright was just installed or updated.       â•‘\nâ•‘ Please run the following command to download new browsers: â•‘\nâ•‘                                                            â•‘\nâ•‘     playwright install                                     â•‘\nâ•‘                                                            â•‘\nâ•‘ <3 Playwright Team                                         â•‘\nâ•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import nest_asyncio\n",
    "from playwright.async_api import async_playwright\n",
    "\n",
    "# Apply nest_asyncio to allow the event loop to be nested\n",
    "nest_asyncio.apply()\n",
    "\n",
    "async def scrape_foodsubs():\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=True)\n",
    "        page = await browser.new_page()\n",
    "        await page.goto(\"https://foodsubs.com/groups\")\n",
    "        await page.wait_for_selector(\"a.card-title\")  # Wait for all links to load\n",
    "\n",
    "        # Extract all the <a> tags with class 'card-title'\n",
    "        links = await page.query_selector_all(\"a.card-title\")\n",
    "\n",
    "        for link in links:\n",
    "            name = await link.inner_text()  # Get the text of the link (ingredient name)\n",
    "            href = await link.get_attribute(\"href\")  # Get the href attribute (ingredient URL)\n",
    "            print(f\"Ingredient: {name}, URL: {href}\")\n",
    "\n",
    "        await browser.close()\n",
    "\n",
    "# Run the function within the existing event loop\n",
    "await scrape_foodsubs()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e794de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting asyncio\n",
      "  Downloading asyncio-3.4.3-py3-none-any.whl.metadata (1.7 kB)\n",
      "Downloading asyncio-3.4.3-py3-none-any.whl (101 kB)\n",
      "Installing collected packages: asyncio\n",
      "Successfully installed asyncio-3.4.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61813ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error while waiting for the page to load: Message: \n",
      "Stacktrace:\n",
      "0   chromedriver                        0x000000010146c2c8 chromedriver + 6197960\n",
      "1   chromedriver                        0x00000001014638ea chromedriver + 6162666\n",
      "2   chromedriver                        0x0000000100ee8de0 chromedriver + 417248\n",
      "3   chromedriver                        0x0000000100f3a797 chromedriver + 751511\n",
      "4   chromedriver                        0x0000000100f3a9b1 chromedriver + 752049\n",
      "5   chromedriver                        0x0000000100f8a9b4 chromedriver + 1079732\n",
      "6   chromedriver                        0x0000000100f609ed chromedriver + 907757\n",
      "7   chromedriver                        0x0000000100f87cdb chromedriver + 1068251\n",
      "8   chromedriver                        0x0000000100f60793 chromedriver + 907155\n",
      "9   chromedriver                        0x0000000100f2cb25 chromedriver + 695077\n",
      "10  chromedriver                        0x0000000100f2d791 chromedriver + 698257\n",
      "11  chromedriver                        0x0000000101428cc0 chromedriver + 5921984\n",
      "12  chromedriver                        0x000000010142cbb1 chromedriver + 5938097\n",
      "13  chromedriver                        0x0000000101403004 chromedriver + 5767172\n",
      "14  chromedriver                        0x000000010142d5db chromedriver + 5940699\n",
      "15  chromedriver                        0x00000001013f1704 chromedriver + 5695236\n",
      "16  chromedriver                        0x00000001014510c8 chromedriver + 6086856\n",
      "17  chromedriver                        0x0000000101451290 chromedriver + 6087312\n",
      "18  chromedriver                        0x00000001014634b1 chromedriver + 6161585\n",
      "19  libsystem_pthread.dylib             0x00007ff80831cdf1 _pthread_start + 99\n",
      "20  libsystem_pthread.dylib             0x00007ff808318857 thread_start + 15\n",
      "\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Set up the WebDriver\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "\n",
    "# Open the URL\n",
    "url = \"https://foodsubs.com/groups\"\n",
    "driver.get(url)\n",
    "\n",
    "# Wait for the page to load completely (adjust time if necessary)\n",
    "# You can also wait for a specific element to load, like a main section\n",
    "try:\n",
    "    # Example of waiting for a specific element to load using XPath\n",
    "    WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.XPATH, '//a[@class=\"card-title\"]'))\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(\"Error while waiting for the page to load:\", e)\n",
    "\n",
    "# Initialize an empty list to store the data\n",
    "substitutes = []\n",
    "\n",
    "# Loop through each group section\n",
    "# Adjust the XPath to target the correct part of the page\n",
    "groups_section = driver.find_elements(By.XPATH, '//a[@class=\"card-title\"]')\n",
    "\n",
    "for group in groups_section:\n",
    "    try:\n",
    "        # Extract the group name (e.g., \"Dairy\" or \"Fruits\")\n",
    "        group_name = group.find_element(By.XPATH, './/h3').text.strip()\n",
    "\n",
    "        # Find all the substitute list items within this group\n",
    "        ingredient_substitutes = group.find_elements(By.XPATH, './/ul/li')\n",
    "\n",
    "        for substitute in ingredient_substitutes:\n",
    "            substitute_name = substitute.text.strip()\n",
    "            substitutes.append({\n",
    "                'Group': group_name,\n",
    "                'Substitute': substitute_name\n",
    "            })\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error while extracting group data:\", e)\n",
    "\n",
    "# Convert the list of dictionaries to a pandas DataFrame\n",
    "df = pd.DataFrame(substitutes)\n",
    "\n",
    "# Save the data to a CSV file\n",
    "df.to_csv('ingredient_substitutes.csv', index=False)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(df.head())\n",
    "\n",
    "# Close the WebDriver after scraping\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "238eb2cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.2.3-cp312-cp312-macosx_10_9_x86_64.whl.metadata (89 kB)\n",
      "Collecting numpy>=1.26.0 (from pandas)\n",
      "  Downloading numpy-2.2.5-cp312-cp312-macosx_14_0_x86_64.whl.metadata (62 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.conda/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in ./.conda/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading pandas-2.2.3-cp312-cp312-macosx_10_9_x86_64.whl (12.5 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.5/12.5 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading numpy-2.2.5-cp312-cp312-macosx_14_0_x86_64.whl (6.7 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hDownloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Installing collected packages: pytz, tzdata, numpy, pandas\n",
      "Successfully installed numpy-2.2.5 pandas-2.2.3 pytz-2025.2 tzdata-2025.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a171081",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
