{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e161885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing data...\n",
      "Loaded 2231141 recipes\n",
      "After cleaning, 2231141 recipes remain\n",
      "Standardizing ingredients...\n",
      "Categorizing recipes...\n",
      "Recipe categories:\n",
      "category\n",
      "other           900297\n",
      "dessert         456009\n",
      "main_protein    408591\n",
      "salad           179333\n",
      "bread           115231\n",
      "soup             81851\n",
      "main_carb        55133\n",
      "breakfast        20913\n",
      "beverage         13783\n",
      "Name: count, dtype: int64\n",
      "Checking for dietary preferences...\n",
      "Vegetarian recipes: 1648142\n",
      "Potentially gluten-free recipes: 1389604\n",
      "Creating recipe feature vectors...\n",
      "Creating recipe embeddings...\n",
      "Generating meal plans...\n",
      "Created 500 balanced meal plans\n",
      "Creating meal plan feature vectors...\n",
      "Creating synthetic ratings...\n",
      "Rating distribution: min=5.63, max=9.48, mean=7.34\n",
      "Training improved model with cross-validation...\n",
      "Epoch 1/50\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 65.1430 - mae: 8.0000 - val_loss: 43.9549 - val_mae: 6.5703\n",
      "Epoch 2/50\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 43.8419 - mae: 6.5030 - val_loss: 28.6580 - val_mae: 5.2459\n",
      "Epoch 3/50\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 27.7640 - mae: 5.0113 - val_loss: 15.5574 - val_mae: 3.7204\n",
      "Epoch 4/50\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 14.5819 - mae: 3.3882 - val_loss: 6.7020 - val_mae: 2.1942\n",
      "Epoch 5/50\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.9022 - mae: 2.4217 - val_loss: 3.7071 - val_mae: 1.6160\n",
      "Epoch 6/50\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.9320 - mae: 2.2160 - val_loss: 3.1807 - val_mae: 1.4761\n",
      "Epoch 7/50\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.3334 - mae: 1.7510 - val_loss: 2.9462 - val_mae: 1.4203\n",
      "Epoch 8/50\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.5854 - mae: 1.9455 - val_loss: 2.7149 - val_mae: 1.3700\n",
      "Epoch 9/50\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.8316 - mae: 1.9388 - val_loss: 2.5378 - val_mae: 1.3136\n",
      "Epoch 10/50\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.1369 - mae: 1.8314 - val_loss: 2.3905 - val_mae: 1.2655\n",
      "Epoch 11/50\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.6636 - mae: 1.7504 - val_loss: 2.2775 - val_mae: 1.2252\n",
      "Epoch 12/50\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.9615 - mae: 1.5900 - val_loss: 2.2330 - val_mae: 1.2103\n",
      "Epoch 13/50\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0712 - mae: 1.5802 - val_loss: 2.1720 - val_mae: 1.1954\n",
      "Epoch 14/50\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.6500 - mae: 1.6807 - val_loss: 2.1030 - val_mae: 1.1747\n",
      "Epoch 15/50\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.9433 - mae: 1.5685 - val_loss: 2.0569 - val_mae: 1.1609\n",
      "Epoch 16/50\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.6329 - mae: 1.7418 - val_loss: 2.1848 - val_mae: 1.1954\n",
      "Epoch 17/50\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.4529 - mae: 1.4537 - val_loss: 1.9826 - val_mae: 1.1197\n",
      "Epoch 18/50\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.5367 - mae: 1.4901 - val_loss: 1.9478 - val_mae: 1.1028\n",
      "Epoch 19/50\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1822 - mae: 1.6746 - val_loss: 1.9089 - val_mae: 1.0832\n",
      "Epoch 20/50\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.4345 - mae: 1.4639 - val_loss: 1.9201 - val_mae: 1.1014\n",
      "Epoch 21/50\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.5440 - mae: 1.4563 - val_loss: 1.9141 - val_mae: 1.0853\n",
      "Epoch 22/50\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.9931 - mae: 1.5546 - val_loss: 1.9339 - val_mae: 1.0930\n",
      "Epoch 23/50\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.8225 - mae: 1.5952 - val_loss: 2.0363 - val_mae: 1.1360\n",
      "Epoch 24/50\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.3940 - mae: 1.4731 - val_loss: 1.9730 - val_mae: 1.1028\n",
      "Evaluating model...\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.4530 - mae: 0.9372 \n",
      "Test Loss (MSE): 1.4471\n",
      "Test MAE: 0.9216\n",
      "Saved training history visualization to training_history.png\n",
      "Generating recommendations...\n",
      "\n",
      "Top 5 Regular Meal Plans:\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
      "Plan 1 (Score: 8.98):\n",
      "  - Cheesy Chicken Spaghetti (main_protein)\n",
      "  - Curried Rice With Shrimp (main_carb)\n",
      "  - Ground Beef And Vegetable Stew (salad)\n",
      "\n",
      "Plan 2 (Score: 8.92):\n",
      "  - Chicken In A Biscuit (main_protein)\n",
      "  - Broccoli, Cheese & Rice Casserole Recipe (main_carb)\n",
      "  - Vegetable Fritatta  (salad)\n",
      "\n",
      "Plan 3 (Score: 8.63):\n",
      "  - Sunday Night Brisket (main_protein)\n",
      "  - Lentils With Rice (main_carb)\n",
      "  - Vegetables With Arugula Broth (salad)\n",
      "\n",
      "Plan 4 (Score: 8.62):\n",
      "  - Cauli-Mash (main_protein)\n",
      "  - Pad Thai Spaghetti Squash “Noodles” (main_carb)\n",
      "  - Waldorf Spinach Salad (salad)\n",
      "\n",
      "Plan 5 (Score: 8.52):\n",
      "  - Sloppy Joes (main_protein)\n",
      "  - Hungarian Noodles with ?Sausage? (main_carb)\n",
      "  - Turkey Salad (salad)\n",
      "\n",
      "\n",
      "Top 3 Vegetarian Meal Plans:\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "Plan 1 (Score: 8.97):\n",
      "  - Pork and Stilton Sausages With Onion Mash\n",
      "  - Simple Tuna Curry With Rice\n",
      "  - Strawberry Spinach Salad \n",
      "\n",
      "Plan 2 (Score: 8.76):\n",
      "  - Tuna Fish Scallop\n",
      "  - 20 Minute Farfalle Pasta And Tuna\n",
      "  - English Pea Salad\n",
      "\n",
      "Plan 3 (Score: 8.43):\n",
      "  - Pork Meatballs With Pineapple Chipotle Salsa\n",
      "  - Spinach and Spicy Ham Pasta Bake\n",
      "  - Simple Egg Salad\n",
      "\n",
      "\n",
      "Top 3 Gluten-Free Meal Plans:\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "Plan 1 (Score: 10.14):\n",
      "  - Chicken Boyah(Makes About 10 To 15 Gallons)  \n",
      "  - Rice Pudding Smoothie\n",
      "  - Pomegranate Fluff Salad\n",
      "\n",
      "Plan 2 (Score: 9.29):\n",
      "  - Party Chicken\n",
      "  - Noodle Pudding(Lochen Kugel)  \n",
      "  - Helen'S Cabbage Salad\n",
      "\n",
      "Plan 3 (Score: 9.19):\n",
      "  - Cornish Pastry(South English Traditional Dish)  \n",
      "  - Red Beans And Rice\n",
      "  - Cabbage Salad\n",
      "\n",
      "\n",
      "Top 3 Vegetarian and Gluten-Free Meal Plans:\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "Plan 1 (Score: 9.43):\n",
      "  - Kalbi (Korean Barbecued Beef Ribs)\n",
      "  - Stir-Fried Rice and Black Quinoa With Cabbage, Red Pepper and Greens\n",
      "  - Tangy Herb Potato Salad\n",
      "\n",
      "Plan 2 (Score: 9.24):\n",
      "  - Meaty Nacho Dip\n",
      "  - Broccoli And Rice Casserole\n",
      "  - Garbanzo Vegetable Stew \n",
      "\n",
      "Plan 3 (Score: 8.82):\n",
      "  - Tandoor Chicken From Savoring India \n",
      "  - Cherry Rice Fluff\n",
      "  - New York Strip With Watercress Salad \n",
      "\n",
      "\n",
      "Summary of Recipe Recommendation System:\n",
      "- Dataset: 2231141 recipes\n",
      "- Generated 500 meal plans for training\n",
      "- Model performance: MAE = 0.92\n",
      "- Support for dietary preferences: Vegetarian and Gluten-free\n",
      "- Features used: TF-IDF ingredient embeddings, category diversity, complexity, nutritional balance\n",
      "\n",
      "Done! The recipe recommendation system has been improved.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid filepath extension for saving. Please add either a `.keras` extension for the native Keras format (recommended) or a `.h5` extension. Use `model.export(filepath)` if you want to export a SavedModel for use with TFLite/TFServing/etc. Received: filepath=recipe_recommender_model.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 444\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mDone! The recipe recommendation system has been improved.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    443\u001b[0m \u001b[38;5;66;03m# Save the trained model and preprocessor for future use\u001b[39;00m\n\u001b[0;32m--> 444\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrecipe_recommender_model\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;66;03m# Save important data for deployment\u001b[39;00m\n\u001b[1;32m    446\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpickle\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/keras/src/saving/saving_api.py:114\u001b[0m, in \u001b[0;36msave_model\u001b[0;34m(model, filepath, overwrite, zipped, **kwargs)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(filepath)\u001b[38;5;241m.\u001b[39mendswith((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.hdf5\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m legacy_h5_format\u001b[38;5;241m.\u001b[39msave_model_to_hdf5(\n\u001b[1;32m    112\u001b[0m         model, filepath, overwrite, include_optimizer\n\u001b[1;32m    113\u001b[0m     )\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid filepath extension for saving. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease add either a `.keras` extension for the native Keras \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat (recommended) or a `.h5` extension. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUse `model.export(filepath)` if you want to export a SavedModel \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor use with TFLite/TFServing/etc. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived: filepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    121\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid filepath extension for saving. Please add either a `.keras` extension for the native Keras format (recommended) or a `.h5` extension. Use `model.export(filepath)` if you want to export a SavedModel for use with TFLite/TFServing/etc. Received: filepath=recipe_recommender_model."
     ]
    }
   ],
   "source": [
    "# Improved Recipe Recommendation System\n",
    "# This notebook contains a more robust implementation of a recipe recommendation system\n",
    "# with better data processing, evaluation, and user experience features\n",
    "\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "import ast\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 1: Load and preprocess data\n",
    "print(\"Loading and preprocessing data...\")\n",
    "\n",
    "# Load the dataset\n",
    "# Note: You'll need to download the dataset first as in your original code\n",
    "df = pd.read_csv(\"recipes_data.csv\")\n",
    "print(f\"Loaded {len(df)} recipes\")\n",
    "\n",
    "# Convert string representations of lists to actual lists (if needed)\n",
    "try:\n",
    "    # Check if ingredients are already lists\n",
    "    if isinstance(df['ingredients'].iloc[0], str):\n",
    "        df['ingredients'] = df['ingredients'].apply(ast.literal_eval)\n",
    "    if isinstance(df['directions'].iloc[0], str):\n",
    "        df['directions'] = df['directions'].apply(ast.literal_eval)\n",
    "except Exception as e:\n",
    "    print(f\"Error converting strings to lists: {e}\")\n",
    "    print(\"Columns may already be in list format\")\n",
    "\n",
    "# Basic data cleaning\n",
    "df.dropna(subset=['title', 'ingredients', 'directions'], inplace=True)\n",
    "print(f\"After cleaning, {len(df)} recipes remain\")\n",
    "\n",
    "# Step 2: Simple Ingredient Standardization\n",
    "print(\"Standardizing ingredients...\")\n",
    "\n",
    "def clean_ingredient(ingredient):\n",
    "    \"\"\"Basic cleaning of ingredient text\"\"\"\n",
    "    # Convert to lowercase\n",
    "    ing = ingredient.lower()\n",
    "    \n",
    "    # Remove quantities (numbers and fractions) and common units\n",
    "    ing = re.sub(r'^[\\d\\s/]+', '', ing)\n",
    "    units = ['cup', 'cups', 'tablespoon', 'tablespoons', 'tbsp', 'teaspoon', 'teaspoons', \n",
    "             'tsp', 'pound', 'pounds', 'lb', 'lbs', 'ounce', 'ounces', 'oz']\n",
    "    \n",
    "    for unit in units:\n",
    "        ing = ing.replace(f\"{unit} \", \" \").replace(f\"{unit}s \", \" \")\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    ing = re.sub(r'\\s+', ' ', ing).strip()\n",
    "    \n",
    "    return ing\n",
    "\n",
    "# Apply cleaning to all ingredients\n",
    "df['clean_ingredients'] = df['ingredients'].apply(lambda x: [clean_ingredient(i) for i in x])\n",
    "\n",
    "# Step 3: Simple Recipe Categorization\n",
    "print(\"Categorizing recipes...\")\n",
    "\n",
    "def simple_categorize_recipe(title, ingredients):\n",
    "    \"\"\"Categorize recipe based on title and ingredients\"\"\"\n",
    "    title = title.lower()\n",
    "    \n",
    "    # Simple classification rules\n",
    "    if any(word in title for word in ['cake', 'cookie', 'pie', 'dessert', 'sweet', 'chocolate', 'ice cream']):\n",
    "        return 'dessert'\n",
    "    elif any(word in title for word in ['breakfast', 'pancake', 'waffle', 'oatmeal', 'cereal']):\n",
    "        return 'breakfast'\n",
    "    elif any(word in title for word in ['salad', 'vegetable']):\n",
    "        return 'salad'\n",
    "    elif any(word in title for word in ['soup', 'stew']):\n",
    "        return 'soup'\n",
    "    elif any(word in title for word in ['chicken', 'beef', 'pork', 'fish', 'meat']):\n",
    "        return 'main_protein'\n",
    "    elif any(word in title for word in ['pasta', 'rice', 'noodle']):\n",
    "        return 'main_carb'\n",
    "    elif any(word in title for word in ['bread', 'muffin', 'roll']):\n",
    "        return 'bread'\n",
    "    elif any(word in title for word in ['drink', 'cocktail', 'smoothie', 'juice']):\n",
    "        return 'beverage'\n",
    "    else:\n",
    "        # Check ingredients for clues\n",
    "        ingredients_str = ' '.join([i.lower() for i in ingredients])\n",
    "        if 'meat' in ingredients_str or 'chicken' in ingredients_str or 'beef' in ingredients_str:\n",
    "            return 'main_protein'\n",
    "        else:\n",
    "            return 'other'\n",
    "\n",
    "# Apply categorization\n",
    "df['category'] = df.apply(lambda x: simple_categorize_recipe(x['title'], x['ingredients']), axis=1)\n",
    "\n",
    "# Display category distribution\n",
    "category_counts = df['category'].value_counts()\n",
    "print(\"Recipe categories:\")\n",
    "print(category_counts)\n",
    "\n",
    "# Step 4: Check for dietary preferences\n",
    "print(\"Checking for dietary preferences...\")\n",
    "\n",
    "def is_vegetarian(ingredients):\n",
    "    \"\"\"Simple check if recipe is vegetarian based on ingredients\"\"\"\n",
    "    ingredients_str = ' '.join([i.lower() for i in ingredients])\n",
    "    meat_keywords = ['chicken', 'beef', 'pork', 'lamb', 'turkey', 'meat', 'fish', 'seafood']\n",
    "    return not any(meat in ingredients_str for meat in meat_keywords)\n",
    "\n",
    "def is_gluten_free(ingredients):\n",
    "    \"\"\"Simple check if recipe might be gluten-free\"\"\"\n",
    "    ingredients_str = ' '.join([i.lower() for i in ingredients])\n",
    "    gluten_keywords = ['flour', 'wheat', 'pasta', 'bread', 'crumb', 'biscuit']\n",
    "    # This is a very simple approximation - in reality, you'd need more sophisticated checking\n",
    "    return not any(gluten in ingredients_str for gluten in gluten_keywords)\n",
    "\n",
    "# Add dietary preference flags\n",
    "df['vegetarian'] = df['ingredients'].apply(is_vegetarian)\n",
    "df['gluten_free'] = df['ingredients'].apply(is_gluten_free)\n",
    "\n",
    "print(f\"Vegetarian recipes: {df['vegetarian'].sum()}\")\n",
    "print(f\"Potentially gluten-free recipes: {df['gluten_free'].sum()}\")\n",
    "\n",
    "# Step 5: Simple Feature Engineering\n",
    "print(\"Creating recipe feature vectors...\")\n",
    "\n",
    "# Function to count ingredients as a complexity metric\n",
    "df['ingredient_count'] = df['ingredients'].apply(len)\n",
    "df['direction_count'] = df['directions'].apply(len)\n",
    "\n",
    "# Calculate a simple complexity score\n",
    "df['complexity'] = (df['ingredient_count'] / df['ingredient_count'].max() + \n",
    "                    df['direction_count'] / df['direction_count'].max()) / 2\n",
    "\n",
    "# Calculate a dietary balance score based on ingredients (very simplified)\n",
    "def simple_nutrition_score(ingredients):\n",
    "    \"\"\"Extremely simplified nutrition scoring\"\"\"\n",
    "    ingredients_str = ' '.join([i.lower() for i in ingredients])\n",
    "    \n",
    "    # Count mentions of various food groups (very simplified)\n",
    "    protein = sum(1 for word in ['meat', 'chicken', 'beef', 'pork', 'fish', 'tofu', 'bean', 'egg', 'nut'] \n",
    "                  if word in ingredients_str)\n",
    "    vegetables = sum(1 for word in ['vegetable', 'carrot', 'broccoli', 'spinach', 'kale', 'potato', 'onion'] \n",
    "                     if word in ingredients_str)\n",
    "    fruits = sum(1 for word in ['fruit', 'apple', 'banana', 'berry', 'orange'] \n",
    "                 if word in ingredients_str)\n",
    "    grains = sum(1 for word in ['grain', 'rice', 'pasta', 'bread', 'oat', 'wheat'] \n",
    "                 if word in ingredients_str)\n",
    "    \n",
    "    # Simple balance score - higher when more food groups are present\n",
    "    food_groups = [protein, vegetables, fruits, grains]\n",
    "    present_groups = sum(1 for group in food_groups if group > 0)\n",
    "    \n",
    "    return present_groups / 4.0  # Normalized to 0-1\n",
    "\n",
    "df['nutrition_balance'] = df['ingredients'].apply(simple_nutrition_score)\n",
    "\n",
    "# Step 6: Simple Bag-of-Words Recipe Embeddings\n",
    "print(\"Creating recipe embeddings...\")\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Create a bag of words representation for ingredients\n",
    "vectorizer = TfidfVectorizer(max_features=100)  # Limit to top features for simplicity\n",
    "ingredient_docs = [' '.join(ing) for ing in df['clean_ingredients']]\n",
    "X_ingredients = vectorizer.fit_transform(ingredient_docs)\n",
    "\n",
    "# Convert sparse matrix to dense array\n",
    "X_ingredients_dense = X_ingredients.toarray()\n",
    "\n",
    "# For each recipe, store its ingredient vector\n",
    "df['embedding'] = list(X_ingredients_dense)\n",
    "\n",
    "# Step 7: Create balanced meal plans\n",
    "print(\"Generating meal plans...\")\n",
    "\n",
    "def create_balanced_meal_plan(df, num_recipes=3, vegetarian=False, gluten_free=False):\n",
    "    \"\"\"Create a balanced meal plan with dietary restrictions\"\"\"\n",
    "    filtered_df = df.copy()\n",
    "    \n",
    "    # Apply dietary filters if requested\n",
    "    if vegetarian:\n",
    "        filtered_df = filtered_df[filtered_df['vegetarian']]\n",
    "    if gluten_free:\n",
    "        filtered_df = filtered_df[filtered_df['gluten_free']]\n",
    "    \n",
    "    if len(filtered_df) < num_recipes:\n",
    "        print(f\"Warning: Not enough recipes match dietary restrictions. Found {len(filtered_df)}\")\n",
    "        return None\n",
    "    \n",
    "    # Try to include main dish, side, and dessert\n",
    "    categories_to_include = ['main_protein', 'main_carb', 'salad', 'dessert']\n",
    "    \n",
    "    plan = []\n",
    "    for category in categories_to_include:\n",
    "        if len(plan) < num_recipes:\n",
    "            category_recipes = filtered_df[filtered_df['category'] == category]\n",
    "            if len(category_recipes) > 0:\n",
    "                plan.append(random.choice(category_recipes.index))\n",
    "    \n",
    "    # Fill remaining slots with random recipes\n",
    "    remaining_recipes = filtered_df.index.difference(plan)\n",
    "    plan.extend(random.sample(list(remaining_recipes), min(num_recipes - len(plan), len(remaining_recipes))))\n",
    "    \n",
    "    return plan[:num_recipes]  # Ensure we return exactly num_recipes\n",
    "\n",
    "# Generate sample meal plans\n",
    "num_plans = 500\n",
    "meal_plans = []\n",
    "for i in range(num_plans):\n",
    "    # Mix of regular, vegetarian, and gluten-free plans\n",
    "    vegetarian = random.random() < 0.3\n",
    "    gluten_free = random.random() < 0.2\n",
    "    \n",
    "    plan = create_balanced_meal_plan(df, num_recipes=3, vegetarian=vegetarian, gluten_free=gluten_free)\n",
    "    if plan:\n",
    "        meal_plans.append(plan)\n",
    "\n",
    "print(f\"Created {len(meal_plans)} balanced meal plans\")\n",
    "\n",
    "# Step 8: Feature vectors for meal plans\n",
    "print(\"Creating meal plan feature vectors...\")\n",
    "\n",
    "def get_meal_plan_features(plan, df):\n",
    "    \"\"\"Create a feature vector for a meal plan\"\"\"\n",
    "    # Get embeddings for each recipe in the plan\n",
    "    recipe_embeddings = [df.loc[idx, 'embedding'] for idx in plan]\n",
    "    \n",
    "    # Average the embeddings\n",
    "    avg_embedding = np.mean(recipe_embeddings, axis=0)\n",
    "    \n",
    "    # Count unique categories as a diversity measure\n",
    "    categories = [df.loc[idx, 'category'] for idx in plan]\n",
    "    category_diversity = len(set(categories)) / len(categories)  # 1.0 = all different\n",
    "    \n",
    "    # Get other features\n",
    "    avg_complexity = np.mean([df.loc[idx, 'complexity'] for idx in plan])\n",
    "    avg_nutrition = np.mean([df.loc[idx, 'nutrition_balance'] for idx in plan])\n",
    "    \n",
    "    # Combine features\n",
    "    additional_features = np.array([category_diversity, avg_complexity, avg_nutrition])\n",
    "    \n",
    "    return np.concatenate([avg_embedding, additional_features])\n",
    "\n",
    "# Create feature vectors for all meal plans\n",
    "X = np.array([get_meal_plan_features(plan, df) for plan in meal_plans])\n",
    "\n",
    "# Step 9: Create synthetic ratings (more realistic than purely random)\n",
    "print(\"Creating synthetic ratings...\")\n",
    "\n",
    "def generate_realistic_rating(plan, df):\n",
    "    \"\"\"Generate a more realistic rating based on properties of the meal plan\"\"\"\n",
    "    # Get categories in the plan\n",
    "    categories = [df.loc[idx, 'category'] for idx in plan]\n",
    "    \n",
    "    # Base score\n",
    "    score = 5.0\n",
    "    \n",
    "    # Bonus for diversity (different categories)\n",
    "    unique_categories = len(set(categories))\n",
    "    score += unique_categories * 0.5\n",
    "    \n",
    "    # Bonus for nutritional balance\n",
    "    nutrition_scores = [df.loc[idx, 'nutrition_balance'] for idx in plan]\n",
    "    avg_nutrition = np.mean(nutrition_scores)\n",
    "    score += avg_nutrition * 2\n",
    "    \n",
    "    # Slight penalty for very complex meals\n",
    "    complexity_scores = [df.loc[idx, 'complexity'] for idx in plan]\n",
    "    avg_complexity = np.mean(complexity_scores)\n",
    "    if avg_complexity > 0.7:  # Only penalize very complex plans\n",
    "        score -= (avg_complexity - 0.7) * 2\n",
    "    \n",
    "    # Add some random noise\n",
    "    score += np.random.normal(0, 0.5)\n",
    "    \n",
    "    # Ensure within range 1-10\n",
    "    return max(1, min(10, score))\n",
    "\n",
    "y = np.array([generate_realistic_rating(plan, df) for plan in meal_plans])\n",
    "\n",
    "print(f\"Rating distribution: min={y.min():.2f}, max={y.max():.2f}, mean={y.mean():.2f}\")\n",
    "\n",
    "# Step 10: Improved model with regularization and cross-validation\n",
    "print(\"Training improved model with cross-validation...\")\n",
    "\n",
    "# First, standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define improved model with regularization\n",
    "def create_model(input_dim):\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=(input_dim,)),\n",
    "        layers.Dense(64, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001)),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(32, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001)),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(1)  # Regression output\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "# Train with early stopping to prevent overfitting\n",
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', patience=5, restore_best_weights=True\n",
    ")\n",
    "\n",
    "model = create_model(X_train.shape[1])\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=50,\n",
    "    batch_size=16,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Step 11: Evaluate model\n",
    "print(\"Evaluating model...\")\n",
    "\n",
    "# Evaluate on test set\n",
    "test_results = model.evaluate(X_test, y_test, verbose=1)\n",
    "print(f\"Test Loss (MSE): {test_results[0]:.4f}\")\n",
    "print(f\"Test MAE: {test_results[1]:.4f}\")\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['mae'])\n",
    "plt.plot(history.history['val_mae'])\n",
    "plt.title('Model MAE')\n",
    "plt.ylabel('MAE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_history.png')\n",
    "plt.close()\n",
    "\n",
    "print(\"Saved training history visualization to training_history.png\")\n",
    "\n",
    "# Step 12: Generate and recommend meal plans based on user preferences\n",
    "print(\"Generating recommendations...\")\n",
    "\n",
    "def recommend_meal_plans(df, model, scaler, num_candidates=100, num_to_recommend=5, \n",
    "                        vegetarian=False, gluten_free=False):\n",
    "    \"\"\"Generate and recommend meal plans based on user preferences\"\"\"\n",
    "    # Generate candidate meal plans\n",
    "    candidate_plans = []\n",
    "    for _ in range(num_candidates):\n",
    "        plan = create_balanced_meal_plan(df, num_recipes=3, vegetarian=vegetarian, gluten_free=gluten_free)\n",
    "        if plan:\n",
    "            candidate_plans.append(plan)\n",
    "    \n",
    "    if len(candidate_plans) == 0:\n",
    "        print(\"Could not generate any meal plans matching preferences\")\n",
    "        return []\n",
    "    \n",
    "    # Create feature vectors\n",
    "    candidate_features = np.array([get_meal_plan_features(plan, df) for plan in candidate_plans])\n",
    "    \n",
    "    # Scale features\n",
    "    candidate_features_scaled = scaler.transform(candidate_features)\n",
    "    \n",
    "    # Predict scores\n",
    "    predicted_scores = model.predict(candidate_features_scaled).flatten()\n",
    "    \n",
    "    # Find top plans\n",
    "    top_indices = np.argsort(predicted_scores)[-num_to_recommend:][::-1]\n",
    "    \n",
    "    # Return top plans with their predicted scores\n",
    "    top_plans = [(candidate_plans[i], predicted_scores[i]) for i in top_indices]\n",
    "    return top_plans\n",
    "\n",
    "# Generate different types of recommendations\n",
    "print(\"\\nTop 5 Regular Meal Plans:\")\n",
    "regular_recommendations = recommend_meal_plans(df, model, scaler)\n",
    "for i, (plan, score) in enumerate(regular_recommendations):\n",
    "    recipes = [df.loc[idx, 'title'] for idx in plan]\n",
    "    categories = [df.loc[idx, 'category'] for idx in plan]\n",
    "    print(f\"Plan {i+1} (Score: {score:.2f}):\")\n",
    "    for j, (recipe, category) in enumerate(zip(recipes, categories)):\n",
    "        print(f\"  - {recipe} ({category})\")\n",
    "    print()\n",
    "\n",
    "print(\"\\nTop 3 Vegetarian Meal Plans:\")\n",
    "veg_recommendations = recommend_meal_plans(df, model, scaler, num_to_recommend=3, vegetarian=True)\n",
    "for i, (plan, score) in enumerate(veg_recommendations):\n",
    "    recipes = [df.loc[idx, 'title'] for idx in plan]\n",
    "    print(f\"Plan {i+1} (Score: {score:.2f}):\")\n",
    "    for recipe in recipes:\n",
    "        print(f\"  - {recipe}\")\n",
    "    print()\n",
    "\n",
    "print(\"\\nTop 3 Gluten-Free Meal Plans:\")\n",
    "gf_recommendations = recommend_meal_plans(df, model, scaler, num_to_recommend=3, gluten_free=True)\n",
    "for i, (plan, score) in enumerate(gf_recommendations):\n",
    "    recipes = [df.loc[idx, 'title'] for idx in plan]\n",
    "    print(f\"Plan {i+1} (Score: {score:.2f}):\")\n",
    "    for recipe in recipes:\n",
    "        print(f\"  - {recipe}\")\n",
    "    print()\n",
    "\n",
    "print(\"\\nTop 3 Vegetarian and Gluten-Free Meal Plans:\")\n",
    "veg_gf_recommendations = recommend_meal_plans(df, model, scaler, num_to_recommend=3, \n",
    "                                            vegetarian=True, gluten_free=True)\n",
    "for i, (plan, score) in enumerate(veg_gf_recommendations):\n",
    "    recipes = [df.loc[idx, 'title'] for idx in plan]\n",
    "    print(f\"Plan {i+1} (Score: {score:.2f}):\")\n",
    "    for recipe in recipes:\n",
    "        print(f\"  - {recipe}\")\n",
    "    print()\n",
    "\n",
    "# Step 13: Summarize recipe recommendation system\n",
    "print(\"\\nSummary of Recipe Recommendation System:\")\n",
    "print(f\"- Dataset: {len(df)} recipes\")\n",
    "print(f\"- Generated {len(meal_plans)} meal plans for training\")\n",
    "print(f\"- Model performance: MAE = {test_results[1]:.2f}\")\n",
    "print(\"- Support for dietary preferences: Vegetarian and Gluten-free\")\n",
    "print(\"- Features used: TF-IDF ingredient embeddings, category diversity, complexity, nutritional balance\")\n",
    "\n",
    "print(\"\\nDone! The recipe recommendation system has been improved.\")\n",
    "\n",
    "# Save the trained model and preprocessor for future use\n",
    "model.save('recipe_recommender_model')\n",
    "# Save important data for deployment\n",
    "import pickle\n",
    "with open('recipe_model_data.pkl', 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'scaler': scaler,\n",
    "        'vectorizer': vectorizer,\n",
    "    }, f)\n",
    "\n",
    "print(\"Model and preprocessors have been saved for future use.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
